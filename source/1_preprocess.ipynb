{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init\n",
    "Import all library here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_filepath = r\"../data/SENSOR/fixed_data/**/*.csv\"\n",
    "sensor_files = glob.glob(sensor_filepath, recursive=True)\n",
    "print(len(sensor_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_sensor_dfs = [ ]\n",
    "for path in sensor_files:\n",
    "    sensor_id = path.split('/')[-2]\n",
    "    df = pd.read_csv(path)\n",
    "    df['SensorCode'] = sensor_id\n",
    "    archive_sensor_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensor_df = pd.concat(map(pd.read_csv, sensor_files))\n",
    "sensor_df = pd.concat(archive_sensor_dfs)\n",
    "del archive_sensor_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.info()\n",
    "sensor_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df['Datetime'] = sensor_df['date'].astype(str) + ' ' + sensor_df['time'].astype(str)\n",
    "sensor_df['Datetime'] = pd.to_datetime(sensor_df['Datetime'], errors='coerce', format=\"%Y-%m-%d %H:%M:%S\")\n",
    "sensor_df.drop(['date', 'time'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read new data\n",
    "New data gathered by my friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sensor_filepath = r'../data/SENSOR/SENSOR(13-08-2022 _ 31-10-2022)/*.csv'\n",
    "new_sensor_files = glob.glob(new_sensor_filepath, recursive=True)\n",
    "print(new_sensor_files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sensor_df = pd.concat(map(pd.read_csv, new_sensor_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sensor_df['Datetime'] = new_sensor_df['Date'].astype(str) + ' ' + new_sensor_df['Time'].astype(str)\n",
    "new_sensor_df['Datetime'] = pd.to_datetime(new_sensor_df['Datetime'], errors='coerce', format=\"%d/%m/%Y %H:%M:%S\")\n",
    "new_sensor_df.drop(['Date', 'Time'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_sensor_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge archive data with new data\n",
    "Into one dataframe only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.rename(columns={ \n",
    "    'latitude':'Latitude', \n",
    "    'longitude':'Longtitude', \n",
    "    'altitude':'Altitude', \n",
    "    'temperature':'Temperature',\n",
    "    'humidity':'Humidity', \n",
    "    'pm1.0':'PM1.0', \n",
    "    'pm2.5':'PM2.5', \n",
    "    'pm10':'PM10', \n",
    "    'uv':'UV', \n",
    "    'co':'CO', \n",
    "    'no2':'NO2', \n",
    "    'so2':'SO2', \n",
    "    'o3':'O3',\n",
    "    'rain':'Rainfall', \n",
    "    'wind_direction':'Direction', \n",
    "    'wind_gust':'WindGust', \n",
    "    'wind_avg':'WindSpeed'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df = pd.concat([sensor_df, new_sensor_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert and clear data\n",
    "Convert date, time to respective format, and delete NANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.drop(['SensorID', 'SensorName'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df['WeekDay'] = sensor_df['Datetime'].dt.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.sort_values(by=['Datetime'], inplace=True)\n",
    "sensor_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df['Direction'] = sensor_df['Direction'].astype(dtype=pd.StringDtype())\n",
    "sensor_df['SensorCode'] = sensor_df['SensorCode'].astype(dtype=pd.StringDtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in sensor_df:\n",
    "    if (sensor_df.dtypes[col] == 'object'):\n",
    "        sensor_df[col] = pd.to_numeric(sensor_df[col], errors='coerce')\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counts = sensor_df.shape[0]\n",
    "sensor_df.dropna(inplace=True)\n",
    "nan_row_count = row_counts - sensor_df.shape[0]\n",
    "row_counts = sensor_df.shape[0]\n",
    "print('Number of NaN rows = ', nan_row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete negative target values: \n",
    "\n",
    "Since the target columns (air pollutant measurements) must be non-negative, and my friend while training got error due to these negative values - these rows must be deleted. I think it's because the sensors got some errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_row_counts = sensor_df.shape[0]\n",
    "row_counts = sensor_df.shape[0]\n",
    "target_columns = ['PM1.0', 'PM2.5', 'PM10', 'UV', 'CO', 'NO2', 'SO2', 'O3']\n",
    "for target in target_columns:\n",
    "    sensor_df.drop(sensor_df[target][ sensor_df[target] < 0 ].index, inplace=True)\n",
    "    print(target, ':', row_counts - sensor_df.shape[0])\n",
    "    row_counts = sensor_df.shape[0]\n",
    "print('Negative target value rows deleted: ', prev_row_counts - sensor_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle wind direction\n",
    "Since Wind direction is string, with the maximum length of 3, I should split this string into 3 seperate colums, each as a category feature. Then apply one-hot encoding or something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from labels to degree (pi based). East direction is 0 and West direction is pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_label_to_pi = {\n",
    "    'X': -1,                        # psudo label, equal NaN?\n",
    "    'N' : 0,                        # remember to treat it as 2*pi when calculate NW and NNW\n",
    "    'NNE': np.pi / 8,\n",
    "    'NE' : np.pi / 4,\n",
    "    'ENE': np.pi * 3 / 8,\n",
    "    'E' : np.pi / 2,\n",
    "    'ESE': np.pi * 5 / 8,\n",
    "    'SE' : np.pi * 3 / 4,\n",
    "    'SSE': np.pi * 7 / 8,\n",
    "    'S' : np.pi,\n",
    "    'SSW': np.pi * 9 / 8,\n",
    "    'SW' : np.pi * 5 / 4,\n",
    "    'WSW': np.pi * 11 / 8,\n",
    "    'W' : np.pi * 3 / 2,\n",
    "    'WNW': np.pi * 13 / 8,\n",
    "    'NW' : np.pi * 7 / 4,           # N as 2*pi, (1.5 + 2)/2 * pi = 1.75 pi\n",
    "    'NNW': np.pi * 15 / 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df['WindDegree'] = sensor_df['Direction'].map(direction_label_to_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell check if the conversion has error. Just had to be sure!\n",
    "\n",
    "wind_direction_valcounts = sensor_df['Direction'].value_counts(sort=True)\n",
    "print(wind_direction_valcounts)\n",
    "wind_degree_valcounts = sensor_df['WindDegree'].value_counts(sort=True)\n",
    "print(wind_degree_valcounts)\n",
    "\n",
    "winddir_valcounts_arr = wind_direction_valcounts.to_numpy()\n",
    "winddegree_valcounts_arr = wind_degree_valcounts.to_numpy()\n",
    "is_winddegree_conversion_error = False\n",
    "for i in range(winddir_valcounts_arr.shape[0]):\n",
    "    if (winddir_valcounts_arr[i] != winddegree_valcounts_arr[i]):\n",
    "        print(i, winddir_valcounts_arr[i], winddegree_valcounts_arr[i])\n",
    "        is_winddegree_conversion_error = True\n",
    "        break\n",
    "if (is_winddegree_conversion_error):\n",
    "    print(\"There's a difference, conversion has error\")\n",
    "else: \n",
    "    print(\"No error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111, polar=True)\n",
    "for id in wind_direction_valcounts.index:\n",
    "    #print(id)\n",
    "    if (id == 'X'): continue\n",
    "    plt.bar(x=direction_label_to_pi[id], height=10*(wind_direction_valcounts[id] / wind_direction_valcounts.sum()), bottom=0, width=np.pi / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df['WindCos'] = np.cos( sensor_df['WindDegree'] )\n",
    "sensor_df['WindSin'] = np.sin( sensor_df['WindDegree'] )\n",
    "sensor_df['WindCos'][ sensor_df['Direction'] == 'X' ] = 0\n",
    "sensor_df['WindSin'][ sensor_df['Direction'] == 'X' ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.drop( ['WindDegree'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary after clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.info()\n",
    "sensor_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the camera csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_filepath = r'../data/CAMERA/CAMERA_CSV(13-08-2022-_31-10-2022)/*.csv'\n",
    "camera_files = glob.glob(camera_filepath, recursive=True)\n",
    "print(camera_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df = pd.concat(map(pd.read_csv, camera_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df.drop(['CameraID', 'CameraName'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df['Datetime'] = camera_df['Date'].astype(str) + ' ' + camera_df['Time'].astype(str)\n",
    "camera_df['Datetime'] = pd.to_datetime(camera_df['Datetime'], errors='coerce', format=\"%d/%m/%y %H:%M:%S\")          # 13/08/22,19:50:02\n",
    "camera_df.drop(['Date', 'Time'], axis = 1, inplace=True)\n",
    "#camera_df['WeekDay'] = camera_df['Datetime'].dt.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_df.sort_values(by=['Datetime'], inplace=True)\n",
    "camera_df.reset_index(inplace=True, drop=True)\n",
    "camera_df.Datetime[ camera_df.Datetime.notna() ].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_df.to_csv('../data/SENSOR/sensors_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M%S\") + '.csv', index=False)\n",
    "camera_df.to_csv('../data/CAMERA/cameras_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M%S\") + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb3de00e07e345391905e8474aaa8d141fe6b310547503c17a51563c220f85dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
