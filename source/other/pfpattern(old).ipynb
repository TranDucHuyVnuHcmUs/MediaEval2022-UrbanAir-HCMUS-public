{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodic frequent-pattern discovery\n",
    "Using algorithm from this paper: https://www.researchgate.net/publication/356825094_Discovering_Periodic-Frequent_Patterns_in_Uncertain_Temporal_Databases?enrichId=rgreq-1de5ac4c085dd4a641f85dda9c527a37-XXX&enrichSource=Y292ZXJQYWdlOzM1NjgyNTA5NDtBUzoxMTQzMTI4MTA4MzQ2MzY1M0AxNjYyNTk3MzM3NTUz&el=1_x_3&_esc=publicationCoverPdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sortednp\n",
    "import datetime\n",
    "import json\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = r\"../data/UTDATABASE/utd_20221222_0226/label.csv\"\n",
    "database_df = pd.read_csv(database_path)\n",
    "database_df.fillna(0, inplace=True)\n",
    "# database_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle time\n",
    "\n",
    "I assume hour can contribute to the traffic jam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df['Datetime'] = pd.to_datetime(database_df['Datetime'], errors='coerce')\n",
    "first_datetime = database_df['Datetime'].min()\n",
    "database_df['Time'] = np.round((database_df['Datetime'] - first_datetime).dt.total_seconds() / 3600)\n",
    "database_df.sort_values(by=['Time'], ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is a time gap that is 120 hours in between, which I suspect is because of data loss. It make all the features have at least their period >= 120. Which I don't think is a good 'bug'. So I planned to delete that time gap. Even though it could lead to data loss, I hope the remaining data can still achieve good patterns. (There's still plenty of days to calculate.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "time_val_counts = database_df['Time'].value_counts()\n",
    "debug_time_df = database_df[ ['Time'] ] \n",
    "debug_time_df['Diff'] = debug_time_df.diff()\n",
    "delete_time_start, max_time_gap = debug_time_df['Time'][ debug_time_df['Diff'].idxmax() ], debug_time_df['Diff'][ debug_time_df['Diff'].idxmax() ]\n",
    "print(delete_time_start, max_time_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# danger! delete time gap\n",
    "database_df = database_df[ database_df['Time'] >= delete_time_start ]\n",
    "database_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_series = database_df.Datetime\n",
    "\n",
    "deleted_columns = ['SensorCode', 'Datetime' ]\n",
    "#database_df.drop([ 'dt' ], axis=1, inplace=True)\n",
    "database_df.drop([ col for col in database_df if col in deleted_columns ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy into another database for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_copy = database_df.copy(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete time related labels\n",
    "\n",
    "Because I think it won't really contribute much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.drop( [ col for col in database_df if ('HourTriple' in col) or ('WeekDay' in col) ], axis=1, inplace=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find frequent items (1-pattern)\n",
    "\n",
    "Both sum of prob and max of time difference (period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database(database_df,\n",
    "                    min_support, max_period):\n",
    "    expsup_one_df = database_df.sum(axis=0)\n",
    "\n",
    "    period_one_df = expsup_one_df.copy(deep=True)\n",
    "    min_time, max_time = database_df['Time'].min(), database_df['Time'].max()\n",
    "    for col in database_df.columns:\n",
    "        if (col == 'Time'): continue\n",
    "        ser = database_df['Time'][ database_df[col] > 0 ]\n",
    "        #print(ser.min(), ser.diff().max(), ser.max())\n",
    "        period_one_df[col] = max( ser.min() - min_time, ser.diff().max(), max_time - ser.max() )\n",
    "\n",
    "    pf_one_items = [ [col, expsup_one_df[col], period_one_df[col]] \n",
    "                        for col in database_df.columns \n",
    "                        if (col != 'Time') and (expsup_one_df[col] >= min_support) and (max_period >= period_one_df[col])  ]\n",
    "    pf_one_df = pd.DataFrame(pf_one_items, columns = ['Item', 'ExpSup', 'MaxPeriod'])\n",
    "    pf_one_df.sort_values(by=['ExpSup', 'MaxPeriod'], ignore_index=True, ascending=False, inplace=True)\n",
    "    pf_one_df\n",
    "\n",
    "    items = pf_one_df['Item'].unique().tolist()\n",
    "    mining_df = database_df[ items + ['Time'] ]\n",
    "    return mining_df, items, pf_one_df, min_time, max_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the table into new format for algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transaction = [ times, ('item', prob) ]. times = Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transactions(mining_df):\n",
    "    transactions = [ None for i in range(mining_df.shape[0]) ]\n",
    "    for i in tqdm(range(mining_df.shape[0])):\n",
    "        item_list = [ ]\n",
    "        for col in mining_df.columns:\n",
    "            if (col == 'Time'): continue\n",
    "            if (mining_df.loc[i, col] > 0):\n",
    "                item_list.append( (col, mining_df.loc[i, col]) )\n",
    "        transactions[i] = [ mining_df.loc[i, 'Time'], item_list ]\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code are taken from: https://github.com/Likhitha-palla/UPFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, item, children, parent, probability=0):\n",
    "        self.item = item\n",
    "        self.probability = probability              # expSupCap of the path/pattern\n",
    "        self.children = children\n",
    "        self.parent = parent\n",
    "        #self.times = []\n",
    "        self.times = np.ndarray(shape=(0))\n",
    "\n",
    "    def addChild(self, node):\n",
    "        if (node.item not in self.children):\n",
    "            self.children[node.item] = []\n",
    "        self.children[node.item] = node\n",
    "        node.parent = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, items):\n",
    "        self.root = Node('Root Node', {}, None)\n",
    "        self.items = items\n",
    "        self.nodelists = {}     # item  : [ nodes of this item ]\n",
    "        for i in items:\n",
    "            self.nodelists[i] = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction(tree: Tree, time, itemlist):\n",
    "    node = tree.root\n",
    "    maxprob = 1\n",
    "    for i in range(len(itemlist)):\n",
    "        item, prob = itemlist[i]         \n",
    "        if (item in node.children):\n",
    "            node = node.children[item]\n",
    "        else:\n",
    "            new_node = Node(item, {}, node)\n",
    "            node.addChild(new_node)\n",
    "            node = new_node\n",
    "            tree.nodelists[item].append(node)\n",
    "        node.probability += prob*maxprob                # multiply by maximum prob of previous items in this transaction.\n",
    "        maxprob = min(maxprob, prob)                    # changing to min, see if it's better\n",
    "    node.times = np.append( node.times, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(transactions, items):\n",
    "    tree = Tree(items)    \n",
    "    for trans in transactions:\n",
    "        add_transaction(tree, trans[0], trans[1])\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove item from tree entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_item(tree: Tree, item):\n",
    "    for node in tree.nodelists[item]:\n",
    "        node.parent.times = np.concatenate([node.parent.times, node.times])\n",
    "        \n",
    "        #sortednp.merge( node.parent.times, node.times ) \n",
    "        #node.parent.times += node.times\n",
    "        node.parent.children[item] = None\n",
    "        del node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPFP-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of patterns shall look like this: [ [ items ], support, period ]\n",
    "\n",
    "The string produced for report for each pattern: {items} [support, period]\n",
    "\n",
    "For example: \"AQI_O3_MED, Motorbike_MED\" [245, 154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_path(node: Node):\n",
    "    path = []\n",
    "    p = node\n",
    "    while (p.parent.item != 'Root Node'):\n",
    "        path.append(p.parent.item)\n",
    "        p = p.parent\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_period(times: np.ndarray, min_time, max_time):\n",
    "    if (times is None) or (len(times) == 0): return 9999999\n",
    "    ts = np.sort(times)\n",
    "    period = max( min(ts) - min_time, max_time - max(ts) )\n",
    "    if (len(ts) > 1):\n",
    "        period = max(period, np.max(np.diff(ts)))\n",
    "    # for i in range(len(ts)-1):\n",
    "    #     difference = ts[i+1] - ts[i]\n",
    "    #     period = max(period, difference)\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_items(cond_patterns, cond_ts, cond_sups, min_support, max_period, min_time, max_time):\n",
    "    item_sup_per = { }\n",
    "    for i in range(len(cond_patterns)):\n",
    "        for item in cond_patterns[i]:\n",
    "            if (item not in item_sup_per):  \n",
    "                item_sup_per[item] = [ 0 , np.ndarray(shape=(0)) ]          # support/prob, period\n",
    "            item_sup_per[item][0] += cond_sups[i]\n",
    "            item_sup_per[item][1] = np.concatenate( [ item_sup_per[item][1], cond_ts[i] ] )  # appending lists\n",
    "    for item in item_sup_per:\n",
    "        item_sup_per[item][1] = find_period(item_sup_per[item][1], min_time, max_time)\n",
    "    freq_item_dict = { key: value for key, value in item_sup_per.items() if (value[0] >= min_support) and (value[1] <= max_period) }\n",
    "    freq_item_dict = dict(sorted(freq_item_dict.items(), key=lambda item: (item[1][0], item[1][1])))\n",
    "\n",
    "    return freq_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition_pattern_base(tree: Tree, item, min_support, max_period, min_time, max_time):\n",
    "    cond_patterns = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_times = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_sups = np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    #[ None for i in range(len(tree.nodelists[item])) ]\n",
    "    \n",
    "    i = 0\n",
    "    for node in tree.nodelists[item]:\n",
    "        cond_patterns[i] = traverse_path(node)\n",
    "        cond_times[i] = node.times \n",
    "        cond_sups[i] = node.probability\n",
    "        i += 1\n",
    "    freq_item_sup_per = find_frequent_items(cond_patterns, cond_times, cond_sups, min_support, max_period, min_time, max_time)\n",
    "    \n",
    "    new_patterns, new_times, new_sups = [], [], np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    count = 0\n",
    "    for p in cond_patterns:\n",
    "        p1 = [ item for item in p if item in freq_item_sup_per ]\n",
    "        if (len(p1) > 0):\n",
    "            p1 = sorted(p1, key=lambda item: (freq_item_sup_per[item][0], freq_item_sup_per[item][1]), reverse=True )\n",
    "            new_patterns.append(p1)\n",
    "            new_times.append(cond_times[count])\n",
    "            #new_sups.append(cond_sups[count])\n",
    "            new_sups[count] = cond_sups[count]\n",
    "        count += 1\n",
    "    new_sups = new_sups[0 : count]\n",
    "    return freq_item_sup_per, new_patterns, new_times, new_sups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_condition(tree: Tree,transaction,times,sup):\n",
    "    node=tree.root\n",
    "    for item in transaction:\n",
    "        if item not in node.children:\n",
    "            new_node=Node(item,{}, node)\n",
    "            node.addChild(new_node)\n",
    "            # if item not in tree.nodelists:\n",
    "            #     tree.nodelists[ item ] = []\n",
    "            tree.nodelists[item].append(new_node)            \n",
    "        node = node.children[item] \n",
    "        node.probability += sup           \n",
    "    node.times = np.concatenate( [node.times, times] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the list of all pattern satisfying the constrains. \n",
    "def upfp_growth(tree: Tree, prefix, \n",
    "                min_support, max_period, \n",
    "                min_time, max_time):\n",
    "    mined_patterns = []\n",
    "\n",
    "    items_list = tree.items\n",
    "    items_list.reverse()            # start from the least frequent item, which have the leaf nodes\n",
    "    for item in items_list:\n",
    "        newprefix = prefix + [item]\n",
    "        expsup = 0\n",
    "        for node in tree.nodelists[item]:\n",
    "            expsup += node.probability\n",
    "        if (expsup >= min_support):\n",
    "            freq_item_sup_per, cond_patterns, cond_times, cond_sups = make_condition_pattern_base(tree, item, \n",
    "                                                                                min_support, max_period, \n",
    "                                                                                min_time, max_time)\n",
    "            cond_tree = Tree(list(freq_item_sup_per.keys()))\n",
    "            for p in range(len(cond_patterns)):\n",
    "                add_transaction_condition(cond_tree, cond_patterns[p], cond_times[p], cond_sups[p])\n",
    "            if (len(cond_patterns) > 0):\n",
    "                mined_patterns += upfp_growth(cond_tree, newprefix, min_support, max_period, min_time, max_time ) \n",
    "            else:\n",
    "                # if no more items to search: stop and return.\n",
    "                mined_patterns.append(newprefix)\n",
    "        remove_item(tree, item)\n",
    "    return mined_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate actual Expected support for one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_patterns(mined_patterns, mining_df, min_time, max_time, min_support, max_period):\n",
    "\n",
    "    max_pattern_length = 0\n",
    "    for ptn in mined_patterns:\n",
    "        max_pattern_length = max(max_pattern_length, len(ptn))\n",
    "    print('Potential max pattern length: ', max_pattern_length)\n",
    "\n",
    "    filtered_patterns = []\n",
    "    for ptn in mined_patterns:\n",
    "        if ( len(ptn) < (max_pattern_length - 3) ): continue\n",
    "        pattern_df = mining_df[ ptn ]\n",
    "\n",
    "        pattern_df['Prob'] = 1\n",
    "        for col in pattern_df.columns:\n",
    "            if (col == 'Prob') or (col == 'Time'): continue\n",
    "            pattern_df['Prob'] *= pattern_df[col]\n",
    "        expSup = pattern_df.Prob.sum()\n",
    "        \n",
    "        time_series = mining_df['Time'][  pattern_df['Prob'] > 0 ] \n",
    "        period = 0\n",
    "        if (time_series is None) or (time_series.shape[0] == 0): \n",
    "            period = 999999\n",
    "        else:\n",
    "            period = max( time_series.min() - min_time, max_time - time_series.max() )\n",
    "            \n",
    "        if (len(time_series) > 1):\n",
    "            period = max( period, time_series.diff().max() )\n",
    "        \n",
    "        if (expSup >= min_support) and (period <= max_period):\n",
    "            #print(expSup, ' ', period)\n",
    "            filtered_patterns.append( [ ptn, expSup, period ] )\n",
    "\n",
    "    max_pattern_length = 0\n",
    "    for ptn in filtered_patterns:\n",
    "        max_pattern_length = max(max_pattern_length, len(ptn[0]))\n",
    "\n",
    "    final_patterns = []\n",
    "    for ptn in filtered_patterns:\n",
    "        if (len(ptn[0]) == max_pattern_length):\n",
    "            final_patterns.append(ptn)\n",
    "\n",
    "    del filtered_patterns\n",
    "\n",
    "    # sort\n",
    "    final_patterns.sort( key = lambda p: (len(p[0]), -p[2], p[1]), reverse=True )\n",
    "    print('Max pattern length: ', max_pattern_length)\n",
    "\n",
    "    pattern_strings = []\n",
    "    for ptn in final_patterns:\n",
    "        item_str = \"\"\n",
    "        for i in range(len(ptn[0]) - 1):\n",
    "            item_str += ptn[0][i] + ', '\n",
    "        item_str += ptn[0][ len(ptn[0]) - 1 ]\n",
    "        item_str += ':[' + str(ptn[1]) + ', ' + str(ptn[2]) + ']\\n' \n",
    "        pattern_strings.append( item_str )\n",
    "    \n",
    "    print('Pattern generated.')\n",
    "    return pattern_strings, final_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(output_folder_path: str, \n",
    "                pattern_strings, pf_one_df, database_path, min_support, max_period):\n",
    "    pf_one_df.to_csv(os.path.join(output_folder_path,'fp_pattern_one.csv'))\n",
    "    with open(os.path.join(output_folder_path, 'patterns.txt'), 'w') as f:\n",
    "        f.writelines(pattern_strings)\n",
    "        f.close()\n",
    "    with open(os.path.join(output_folder_path, 'setting.json'), 'w') as f:\n",
    "        f.write( json.dumps({ 'data_path': database_path,  'min_support': min_support, 'max_period': max_period }, indent=4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual run\n",
    "\n",
    "With sensor + traffic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_patterns(database_df, output_folder_path, min_support, max_period):\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    mining_df, item_list, pf_one_df, min_time, max_time = prepare_database(database_df, min_support, max_period)\n",
    "    transactions = make_transactions(mining_df)\n",
    "    tree = build_tree(transactions, item_list)\n",
    "    mined_patterns = upfp_growth(tree, [], min_support, max_period, min_time, max_time )\n",
    "    pattern_strings, _ = produce_patterns(mined_patterns, mining_df, min_time, max_time, min_support, max_period)\n",
    "    export_results(output_folder_path, pattern_strings, pf_one_df, database_path, min_support, max_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_default_support = 500\n",
    "max_default_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = '../output_temp/FULLDATA/ufp_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "mine_patterns(database_df, output_folder_path, min_default_support, max_default_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom mining for other purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = ['Altitude', 'Temperature', 'Humidity', 'Rainfall', 'WindGust', 'WindSpeed', 'WindCos', 'WindSin', 'UV']\n",
    "traffic_columns = ['Person', 'Car', 'Bus', 'Motorbike', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns(database_df, columns):\n",
    "    column_names = []\n",
    "    for c in database_df.columns:\n",
    "        for l in columns:\n",
    "            if l in c:\n",
    "                column_names.append(c)\n",
    "                break\n",
    "    #print(column_names)\n",
    "    new_database_df = database_df.drop( column_names, axis=1 ).reset_index(drop=True)\n",
    "    #new_database_df.info()\n",
    "    return new_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_traffic_support = 10\n",
    "max_traffic_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_traffic_folder_path = r'../output_temp/TRAFFIC/ufp_traffic_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_traffic_folder_path, exist_ok=True)\n",
    "\n",
    "traffic_database_df = delete_columns(database_df, sensor_columns)\n",
    "mine_patterns(traffic_database_df, output_traffic_folder_path,\n",
    "            min_traffic_support, max_traffic_period)\n",
    "# del traffic_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_sensor_folder_path = '../data/UTDatabase/Mining/ufp_sensor_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "# os.makedirs(output_sensor_folder_path, exist_ok=True)\n",
    "\n",
    "# sensor_database_df = delete_columns(database_df, traffic_columns)\n",
    "# mine_patterns(sensor_database_df, output_sensor_folder_path, min_support, max_period)\n",
    "# del sensor_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with specific columns for traffic relation finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to see if for extreme cases such as PM10 lv4, is there a hidden pattern among them?\n",
    "Each columns shall be selected, and the dataframe will be filtered based on that col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_spec_support = 3\n",
    "max_spec_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_database_df = delete_columns(database_df, sensor_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_unhelpful_columns = ['CO', 'NO2', 'SO2']\n",
    "# dropping_unhelpful_columns = []\n",
    "# for col in traffic_database_df.columns:\n",
    "#     for ex in exclude_unhelpful_columns:\n",
    "#         if (ex in col) :\n",
    "#             dropping_unhelpful_columns.append(col)\n",
    "# new_traffic_database_df = traffic_database_df.drop( dropping_unhelpful_columns , axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in new_traffic_database_df.columns:\n",
    "#     if ('HIGH' in col) or ('lv4' in col) or ('lv5' in col) or ('HourTriple' in col): \n",
    "#         sub_df = new_traffic_database_df[ new_traffic_database_df[col] > 0 ].copy(True).reset_index(drop=True)\n",
    "#         output_col_path = os.path.join(output_speccol_path, col)\n",
    "#         mine_patterns(sub_df, output_col_path, min_spec_support, max_spec_period )\n",
    "#         del sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with each one AQI item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration, isolate one AQI item from the rest (delete all other AQI items), to find pattern specifically for that item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_AQI_support = 1\n",
    "max_AQI_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_label_columns = [ col for col in traffic_database_df.columns if 'AQI' in col ]\n",
    "aqi_label_columns\n",
    "\n",
    "aqi_df = traffic_database_df[ aqi_label_columns ].copy(True)\n",
    "\n",
    "custom_aqi_database_df = traffic_database_df.drop( aqi_label_columns, axis=1 )\n",
    "custom_aqi_database_df = custom_aqi_database_df.drop( [ c for c in custom_aqi_database_df.columns if 'WeekDay' in c ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_aqi_path = r\"../output_temp/AQI/ufp_aqi_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_aqi_path, exist_ok=True)\n",
    "\n",
    "for aqicol in aqi_label_columns:\n",
    "    print(aqicol)\n",
    "    sub_df = custom_aqi_database_df.copy(True)\n",
    "    sub_df[aqicol] = aqi_df[aqicol].copy(True)\n",
    "    sub_df = sub_df[ sub_df[aqicol] > 0 ].sort_values(by=['Time'], ignore_index=True)               # prob = 0 mean not exist, dont use\n",
    "    if (sub_df.shape[0] == 0): continue\n",
    "    output_col_path = os.path.join(output_aqi_path, aqicol)\n",
    "    mine_patterns(sub_df, output_col_path, min_AQI_support, max_AQI_period )\n",
    "    del sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only high traffic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hightraffic_support = 15\n",
    "max_hightraffic_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_traffic_database_df = traffic_database_df\n",
    "# for col in traffic_columns:\n",
    "#     high_traffic_database_df = high_traffic_database_df[ high_traffic_database_df[ col + '_LOW' ] == 0 ]\n",
    "# high_traffic_database_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_hightraffic_path = r\"../output_temp/HIGHTRAFFIC/ufp_hightraffic_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "# os.makedirs(output_hightraffic_path, exist_ok=True)\n",
    "\n",
    "# mine_patterns(high_traffic_database_df, output_hightraffic_path, min_hightraffic_support, max_hightraffic_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining with each traffic item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process to each AQI item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eachtraffic_support = 1.5\n",
    "max_eachtraffic_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_label_columns = []\n",
    "for tcol in traffic_columns:\n",
    "    for col in traffic_database_df.columns:\n",
    "        if (tcol in col):\n",
    "            traffic_label_columns.append(col)\n",
    "\n",
    "trafficonly_df = traffic_database_df[ traffic_label_columns ].copy(True)\n",
    "traffic_excluded_database_df = traffic_database_df.drop( traffic_label_columns, axis=1 )\n",
    "# traffic_excluded_database_df = traffic_excluded_database_df.drop( [ c for c in traffic_excluded_database_df.columns if 'WeekDay' in c ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_excluded_database_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eachtraffic_path = r\"../output_temp/EACHTRAFFIC/ufp_eachtraffic_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_eachtraffic_path, exist_ok=True)\n",
    "\n",
    "for c in traffic_label_columns:\n",
    "    print(c)\n",
    "    sub_df = traffic_excluded_database_df.copy(True)\n",
    "    sub_df[c] = trafficonly_df[c].copy(True)\n",
    "    sub_df = sub_df[ sub_df[c] > 0 ].sort_values(by=['Time'], ignore_index=True)               # prob = 0 mean not exist, dont use\n",
    "    if (sub_df.shape[0] == 0): continue\n",
    "    output_col_path = os.path.join(output_eachtraffic_path, c)\n",
    "    mine_patterns(sub_df, output_col_path, min_eachtraffic_support, max_eachtraffic_period )\n",
    "    del sub_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb3de00e07e345391905e8474aaa8d141fe6b310547503c17a51563c220f85dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
