{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodic frequent-pattern discovery\n",
    "Using algorithm from this paper: https://www.researchgate.net/publication/356825094_Discovering_Periodic-Frequent_Patterns_in_Uncertain_Temporal_Databases?enrichId=rgreq-1de5ac4c085dd4a641f85dda9c527a37-XXX&enrichSource=Y292ZXJQYWdlOzM1NjgyNTA5NDtBUzoxMTQzMTI4MTA4MzQ2MzY1M0AxNjYyNTk3MzM3NTUz&el=1_x_3&_esc=publicationCoverPdf\n",
    "\n",
    "\n",
    "Its source code can be found here: https://github.com/Likhitha-palla/UPFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib\n",
    "import sortednp\n",
    "import datetime\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import timeit\n",
    "import time\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find frequent items (1-pattern)\n",
    "\n",
    "Both sum of prob and max of time difference (period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database(database_df,\n",
    "                    min_support, max_period):\n",
    "    expsup_one_df = database_df.sum(axis=0)\n",
    "\n",
    "    period_one_df = expsup_one_df.copy(deep=True)\n",
    "    min_time, max_time = database_df['Time'].min(), database_df['Time'].max()\n",
    "    for col in database_df.columns:\n",
    "        if (col == 'Time'): continue\n",
    "        ser = database_df['Time'][ database_df[col] > 0 ]\n",
    "        #print(ser.min(), ser.diff().max(), ser.max())\n",
    "        period_one_df[col] = max( ser.min() - min_time, ser.diff().max(), max_time - ser.max() )\n",
    "\n",
    "    pf_one_items = [ [col, expsup_one_df[col], period_one_df[col]] \n",
    "                        for col in database_df.columns \n",
    "                        if (col != 'Time') and (expsup_one_df[col] >= min_support) and (max_period >= period_one_df[col])  ]\n",
    "    pf_one_df = pd.DataFrame(pf_one_items, columns = ['Item', 'ExpSup', 'MaxPeriod'])\n",
    "    pf_one_df.sort_values(by=['ExpSup', 'MaxPeriod'], ignore_index=True, ascending=False, inplace=True)\n",
    "    pf_one_df\n",
    "\n",
    "    items = pf_one_df['Item'].unique().tolist()\n",
    "    mining_df = database_df[ ['Time'] + items ]\n",
    "\n",
    "    #print('Items = ', items)\n",
    "    return mining_df, items, pf_one_df, min_time, max_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, item, children, parent, probability=0):\n",
    "        self.item = item\n",
    "        self.probability = probability              # expSupCap of the path/pattern\n",
    "        self.children = children\n",
    "        self.parent = parent\n",
    "        #self.times = []\n",
    "        self.times = np.ndarray(shape=(0))\n",
    "\n",
    "    def addChild(self, node):\n",
    "        if (node.item not in self.children):\n",
    "            self.children[node.item] = []\n",
    "        self.children[node.item] = node\n",
    "        node.parent = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, items):\n",
    "        self.nodecounts = 0\n",
    "        self.root = Node(-1, {}, None)\n",
    "        self.items = items\n",
    "        self.nodelists = {}     # item  : [ nodes of this item ]\n",
    "        for i in items:\n",
    "            self.nodelists[i] = []\n",
    "        # self.nodelists = [ [] for i in range(len(items)) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(matrix, time_col, items, algo_mode: int = 1):\n",
    "    tree = Tree(items)    \n",
    "    print(\"Algo mode = \", algo_mode, \" (1 = min, 2 = max)\")\n",
    "    \n",
    "    x_indexes, y_indexes = np.where( matrix > 0 )\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    i = 0\n",
    "    node = tree.root\n",
    "    item = items[y_indexes[0]]\n",
    "    expCap = 1\n",
    "        \n",
    "        \n",
    "    while (i < x_indexes.shape[0]):\n",
    "        x_idx = x_indexes[i]        # row\n",
    "        y_idx = y_indexes[i]        # col\n",
    "\n",
    "        item = items[y_idx]\n",
    "        #item = y_idx                # now use the index as the item itself (the list of item names are present in the tree already)\n",
    "        prob = matrix[x_idx][y_idx]\n",
    "        \n",
    "        if (item in node.children):\n",
    "            node = node.children[item]\n",
    "        else:\n",
    "            new_node = Node(item, {}, node)\n",
    "            tree.nodecounts += 1\n",
    "            node.addChild(new_node)\n",
    "            node = new_node\n",
    "            tree.nodelists[item].append(node)\n",
    "        node.probability += prob*expCap                # multiply by maximum prob of previous items in this transaction.\n",
    "        \n",
    "        if (algo_mode == 1):\n",
    "            expCap = min(expCap, prob)\n",
    "        elif (algo_mode == 2):\n",
    "            expCap = max(expCap, prob)                    # changing to max, see if it's better\n",
    "    \n",
    "        i += 1\n",
    "        if (i >= x_indexes.shape[0]) or (x_idx != x_indexes[i]):               # the current row is done, the node is the leaf\n",
    "            node.times = np.append( node.times, time_col[x_idx] )      \n",
    "            node = tree.root            # reset back to root\n",
    "            expCap = 1\n",
    "        #progress_bar.update(1)\n",
    "    \n",
    "    #progress_bar.close()\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove item from tree entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_item(tree: Tree, item):\n",
    "    for node in tree.nodelists[item]:\n",
    "        node.parent.times = np.concatenate([node.parent.times, node.times])\n",
    "        \n",
    "        #sortednp.merge( node.parent.times, node.times ) \n",
    "        #node.parent.times += node.times\n",
    "        node.parent.children[item] = None\n",
    "        del node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPFP-growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_path(node: Node):\n",
    "    path = []\n",
    "    p = node\n",
    "    while (p.parent.item != -1):\n",
    "        path.append(p.parent.item)\n",
    "        p = p.parent\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_period(times: np.ndarray, min_time: float, max_time: float):\n",
    "    if (times is None) or (len(times) == 0): return 9999999\n",
    "    ts = np.sort(times)\n",
    "    period = max( min(ts) - min_time, max_time - max(ts) )\n",
    "    if (len(ts) > 1):\n",
    "        period = max(period, np.max(np.diff(ts)))\n",
    "    # for i in range(len(ts)-1):\n",
    "    #     difference = ts[i+1] - ts[i]\n",
    "    #     period = max(period, difference)\n",
    "    return period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_items(cond_patterns, cond_ts, cond_sups, min_support, max_period, min_time, max_time):\n",
    "    item_sup_per = { }\n",
    "    for i in range(len(cond_patterns)):\n",
    "        for item in cond_patterns[i]:\n",
    "            if (item not in item_sup_per):  \n",
    "                item_sup_per[item] = [ 0 , np.ndarray(shape=(0)) ]          # support/prob, period\n",
    "            item_sup_per[item][0] += cond_sups[i]\n",
    "            item_sup_per[item][1] = np.concatenate( [ item_sup_per[item][1], cond_ts[i] ] )  # appending lists\n",
    "    for item in item_sup_per:\n",
    "        item_sup_per[item][1] = find_period(item_sup_per[item][1], min_time, max_time)\n",
    "    freq_item_dict = { key: value for key, value in item_sup_per.items() if (value[0] >= min_support) and (value[1] <= max_period) }\n",
    "    freq_item_dict = dict(sorted(freq_item_dict.items(), key=lambda item: (item[1][0], item[1][1])))\n",
    "\n",
    "    return freq_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition_pattern_base(tree: Tree, item, min_support, max_period, min_time, max_time):\n",
    "    cond_patterns = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_times = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_sups = np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    #[ None for i in range(len(tree.nodelists[item])) ]\n",
    "    \n",
    "    i = 0\n",
    "    for node in tree.nodelists[item]:\n",
    "        cond_patterns[i] = traverse_path(node)\n",
    "        cond_times[i] = node.times \n",
    "        cond_sups[i] = node.probability\n",
    "        i += 1\n",
    "    freq_item_sup_per = find_frequent_items(cond_patterns, cond_times, cond_sups, min_support, max_period, min_time, max_time)\n",
    "    \n",
    "    new_patterns, new_times, new_sups = [], [], np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    count = 0\n",
    "    for p in cond_patterns:\n",
    "        p1 = [ item for item in p if item in freq_item_sup_per ]\n",
    "        if (len(p1) > 0):\n",
    "            p1 = sorted(p1, key=lambda item: (freq_item_sup_per[item][0], freq_item_sup_per[item][1]), reverse=True )\n",
    "            new_patterns.append(p1)\n",
    "            new_times.append(cond_times[count])\n",
    "            #new_sups.append(cond_sups[count])\n",
    "            new_sups[count] = cond_sups[count]\n",
    "        count += 1\n",
    "    new_sups = new_sups[0 : count]\n",
    "    return freq_item_sup_per, new_patterns, new_times, new_sups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_condition(tree: Tree,transaction,times,sup):\n",
    "    node=tree.root\n",
    "    for item in transaction:\n",
    "        if item not in node.children:\n",
    "            new_node=Node(item,{}, node)\n",
    "            node.addChild(new_node)\n",
    "            # if item not in tree.nodelists:\n",
    "            #     tree.nodelists[ item ] = []\n",
    "            tree.nodelists[item].append(new_node)            \n",
    "        node = node.children[item] \n",
    "        node.probability += sup           \n",
    "    node.times = np.concatenate( [node.times, times] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the list of all pattern satisfying the constrains. \n",
    "def upfp_growth(tree: Tree, prefix, \n",
    "                min_support, max_period, \n",
    "                min_time, max_time):\n",
    "    mined_patterns = []\n",
    "\n",
    "    for i in range(len(tree.items)-1, -1, -1):\n",
    "        item = tree.items[i]\n",
    "        newprefix = prefix + [item]\n",
    "        expsup = 0\n",
    "        for node in tree.nodelists[item]:\n",
    "            expsup += node.probability\n",
    "        if (expsup >= min_support):\n",
    "            freq_item_sup_per, cond_patterns, cond_times, cond_sups = make_condition_pattern_base(tree, item, \n",
    "                                                                                min_support, max_period, \n",
    "                                                                                min_time, max_time)\n",
    "            cond_tree = Tree(list(freq_item_sup_per.keys()))\n",
    "            for p in range(len(cond_patterns)):\n",
    "                add_transaction_condition(cond_tree, cond_patterns[p], cond_times[p], cond_sups[p])\n",
    "            if (len(cond_patterns) > 0):\n",
    "                mined_patterns += upfp_growth(cond_tree, newprefix, min_support, max_period, min_time, max_time ) \n",
    "            else:\n",
    "                # if no more items to search: stop and return.\n",
    "                mined_patterns.append(np.array(newprefix, dtype=np.int16))\n",
    "        remove_item(tree, item)\n",
    "    return mined_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate actual Expected support for one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_sort(array, count_func):\n",
    "    max_count = 0\n",
    "    for i in range(len(array)):\n",
    "        max_count = max(max_count, count_func(array[i]))\n",
    "\n",
    "    # the usable count is [0, max_count], inclusive, hence (max_count + 1) different counts\n",
    "    # we need h[max_count+1] be the end index of sorted array, for iterating the last count's indexes\n",
    "    count_list_heads = [ 0 for i in range(max_count + 2)]          \n",
    "    for i in range(len(array)):\n",
    "        count_list_heads[ count_func(array[i]) ] += 1\n",
    "    \n",
    "    # cumulative sum\n",
    "    for i in range(max_count + 1):\n",
    "        count_list_heads[i + 1] += count_list_heads[i]\n",
    "    \n",
    "    sorted_array = [ None for i in range(len(array)) ]\n",
    "    for i in range(len(array)):\n",
    "        count = count_func(array[i])\n",
    "        count_list_heads[ count ] -= 1\n",
    "        sorted_array[ count_list_heads[ count ] ] = array[i] \n",
    "\n",
    "    return sorted_array, count_list_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def pattern_priority(ptn):\n",
    "    return np.min(ptn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_expSup(pattern_matrix: np.ndarray):\n",
    "    probabilities = np.zeros(pattern_matrix.shape[0])\n",
    "    for i in range(pattern_matrix.shape[0]):\n",
    "        probabilities[i] = np.prod(pattern_matrix[i, :])\n",
    "    # probabilities = np.prod(pattern_matrix, axis=1, keepdim=False)\n",
    "    return probabilities, np.sum(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_final_period(times: np.ndarray, \n",
    "                    probabilities: np.ndarray, \n",
    "                    min_time: float, max_time: float):\n",
    "    period = 0\n",
    "    times = times.copy()\n",
    "    times = times[ probabilities > 0 ]\n",
    "    # times = times.to('cuda:0')\n",
    "    if (times is None) or (times.shape[0] == 0): \n",
    "        period = 999999\n",
    "    else:\n",
    "        period = max( np.min(times) - min_time, max_time - np.max(times) )\n",
    "    if (len(times) > 1):\n",
    "        period = max( period, np.max(np.diff(times)) )\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_patterns(mined_patterns, mining_df, min_time, max_time, min_support, max_period):\n",
    "    \n",
    "    sorted_patterns, length_heads = counting_sort(mined_patterns, lambda ptn: len(ptn))\n",
    "    for length in range(1, len(length_heads) - 2):\n",
    "        sorted_patterns[ length_heads[length] : length_heads[length+1] ] = sorted(\n",
    "                                    sorted_patterns[length_heads[length] : length_heads[length+1]],\n",
    "                                    key = lambda p: pattern_priority(p) )\n",
    "\n",
    "    filtered_patterns = []\n",
    "    print('Potential max pattern length = ', len(length_heads) - 2)\n",
    "    \n",
    "    tsr = mining_df.drop(columns=['Time'], axis=1).to_numpy()\n",
    "    time_tsr = mining_df['Time'].to_numpy()\n",
    "    #tsr = torch.FloatTensor( mining_df.drop(columns=['Time'], axis=1).to_numpy() ).detach().to(device)      # for faster calculation for big dataset\n",
    "    #time_tsr = torch.LongTensor( mining_df['Time'].to_numpy()).detach().to(device)\n",
    "    \n",
    "\n",
    "    # binary search\n",
    "    min_length = 1\n",
    "    max_length = len(length_heads) - 2\n",
    "    length = 0\n",
    "    start_pattern_idx = -1\n",
    "    max_pattern_start_idx = -1\n",
    "\n",
    "\n",
    "    # quick search to reduce the space needed to scan.\n",
    "    length = 1\n",
    "    while (length <= max_length):\n",
    "        pattern_idx = length_heads[length]\n",
    "        start_pattern_idx = -1\n",
    "        while (pattern_idx < length_heads[length + 1]):\n",
    "            ptn = sorted_patterns[pattern_idx]  \n",
    "            #sub_tsr = torch.index_select(input=tsr, dim=1, index=torch.LongTensor(ptn).to(device))\n",
    "            sub_tsr = tsr[ :, ptn ]\n",
    "            probabilities, expSup = find_expSup(sub_tsr)\n",
    "            period = find_final_period(time_tsr, probabilities,\n",
    "                                min_time, max_time)\n",
    "            if (expSup >= min_support) and (period <= max_period):       \n",
    "                start_pattern_idx = pattern_idx\n",
    "                break\n",
    "            #torch.cuda.empty_cache() \n",
    "            if (length_heads[length + 1] - length_heads[length] == 1):\n",
    "                break\n",
    "            pattern_idx += int((np.log2(length_heads[length + 1] - length_heads[length])))\n",
    "            \n",
    "        if (start_pattern_idx > -1):\n",
    "            print(\"Found the pattern with length = \", length)\n",
    "            max_pattern_length = length\n",
    "            max_pattern_start_idx = length_heads[length]\n",
    "            min_length = length + 1\n",
    "        length *= 2\n",
    "\n",
    "\n",
    "    # this search find the most possible length, but you must pass it another time to have all patterns with such length.\n",
    "    while (min_length <= max_length):\n",
    "        start_pattern_idx = -1\n",
    "        length = (min_length + max_length) // 2\n",
    "        print(\"Checking potential patterns with length = \", length)\n",
    "\n",
    "        for pattern_idx in tqdm(range( length_heads[length], length_heads[ length + 1 ]), desc=\"pattern\" ):\n",
    "            ptn = sorted_patterns[pattern_idx]\n",
    "            sub_tsr = tsr[ :, ptn ]\n",
    "            probabilities, expSup = find_expSup(sub_tsr)\n",
    "            period = find_final_period(time_tsr, probabilities,\n",
    "                                min_time, max_time)\n",
    "\n",
    "            if (expSup >= min_support) and (period <= max_period):       \n",
    "                start_pattern_idx = pattern_idx\n",
    "                break\n",
    "\n",
    "        if (start_pattern_idx > -1):\n",
    "            print(\"Found the pattern with length = \", length)\n",
    "            max_pattern_length = length\n",
    "            max_pattern_start_idx = start_pattern_idx\n",
    "            min_length = length + 1\n",
    "        else:\n",
    "            max_length = length - 1\n",
    "        \n",
    "\n",
    "    print(\"Now calculating all patterns with max length...\")\n",
    "    for pattern_idx in tqdm(range( max_pattern_start_idx, length_heads[ max_pattern_length + 1 ]), desc=\"pattern\" ):\n",
    "        ptn = sorted_patterns[pattern_idx]\n",
    "        sub_tsr = tsr[ :, ptn ]\n",
    "        probabilities, expSup = find_expSup(sub_tsr)\n",
    "        period = find_final_period(time_tsr, probabilities,\n",
    "                            min_time, max_time)\n",
    "        if (expSup >= min_support) and (period <= max_period):          \n",
    "            filtered_patterns.append( [ ptn, expSup, period ] ) \n",
    "        \n",
    "\n",
    "    filtered_patterns.sort( key = lambda p: (len(p[0]), -p[2], p[1]), reverse=True )\n",
    "    return filtered_patterns, max_pattern_length, len(length_heads) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_strings_from_patterns(patterns, item_list):\n",
    "    pattern_strings = []\n",
    "    for ptn in patterns:\n",
    "        item_str = \"\"\n",
    "        for i in range(len(ptn[0]) - 1):\n",
    "            item_str += item_list[ptn[0][i]] + ', '\n",
    "        item_str += item_list[ptn[0][ len(ptn[0]) - 1 ]]\n",
    "        item_str += ':[' + str(ptn[1]) + ', ' + str(ptn[2]) + ']\\n' \n",
    "        pattern_strings.append( item_str )\n",
    "    return pattern_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_patterns(mined_patterns, mining_df, item_list, min_time, max_time, min_support, max_period):\n",
    "\n",
    "    final_patterns, max_pattern_length, potential_max_length = filter_patterns(mined_patterns, mining_df, \n",
    "                                min_time, max_time, min_support, max_period)\n",
    "    #print('Max pattern length: ', max_pattern_length)\n",
    "    pattern_strings = make_strings_from_patterns(final_patterns, item_list)\n",
    "    if (len(pattern_strings) > 0): print('Most periodic pattern = ', pattern_strings[0])\n",
    "    return pattern_strings, final_patterns, max_pattern_length, potential_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(output_folder_path: str, \n",
    "                pattern_strings, pf_one_df, database_path, min_support, max_period):\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    pf_one_df.to_csv(os.path.join(output_folder_path,'fp_pattern_one.csv'))\n",
    "    with open(os.path.join(output_folder_path, 'patterns.txt'), 'w') as f:\n",
    "        f.writelines(pattern_strings)\n",
    "        f.close()\n",
    "    with open(os.path.join(output_folder_path, 'setting.json'), 'w') as f:\n",
    "        f.write( json.dumps({ 'data_path': database_path,  'min_support': min_support, 'max_period': max_period }, indent=4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable_columns = [ 'Mode', 'MinSup', 'MaxPer', \n",
    "                'PFItemCount', \n",
    "                'prepare_database', 'make_transaction', 'build_tree', 'upfp_growth', 'produce_pattern', \n",
    "                'max_pattern_length', 'potential_max_length', 'potential_pattern_count',\n",
    "                'size_tree' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_patterns(database_df, output_folder_path, \n",
    "                min_support, max_period, algo_mode: int = 1, export: bool = True):\n",
    "    \n",
    "    times = [ 0 for i in range(5) ]\n",
    "    \n",
    "    times[0] = timeit.default_timer()\n",
    "    mining_df, item_list, pf_one_df, min_time, max_time = prepare_database(database_df, min_support, max_period)\n",
    "    times[0] = timeit.default_timer() - times[0]\n",
    "    \n",
    "    if (len(item_list) == 0): \n",
    "        return [], pf_one_df, None\n",
    "        \n",
    "    times[2] = timeit.default_timer()\n",
    "    #tree = build_tree(transactions, item_list, algo_mode)\n",
    "    time_col = mining_df['Time'].copy().to_numpy()         # to ensure no editing shall affect the main data\n",
    "    npmatrix = mining_df.drop(columns=['Time'], axis=1).to_numpy()\n",
    "    tree = build_tree(npmatrix, time_col, list(range(len(item_list))), algo_mode)\n",
    "    times[2] = timeit.default_timer() - times[2]\n",
    "    size_tree = tree.nodecounts\n",
    "    print(\"Tree node count = \", size_tree)\n",
    "\n",
    "    times[3] = timeit.default_timer()\n",
    "    mined_patterns = upfp_growth(tree, [], min_support, max_period, min_time, max_time )\n",
    "    times[3] = timeit.default_timer() - times[3]\n",
    "    potential_pattern_counts = len(mined_patterns)\n",
    "    print(\"Algo time = \", times[3])\n",
    "\n",
    "    max_pattern_length, potential_max_length = -1, -1\n",
    "    times[4] = timeit.default_timer()\n",
    "    if (export):\n",
    "        pattern_strings, _, max_pattern_length, potential_max_length = produce_patterns(mined_patterns, mining_df, item_list,\n",
    "                                                            min_time, max_time, min_support, max_period)\n",
    "        export_results(output_folder_path, pattern_strings, pf_one_df, database_path, min_support, max_period)\n",
    "    else: pattern_strings = \"\"\n",
    "    times[4] = timeit.default_timer() - times[4]\n",
    "\n",
    "    times = [algo_mode, min_support, max_period, len(item_list) ] + times + [max_pattern_length, potential_max_length, potential_pattern_counts, size_tree]\n",
    "    \n",
    "    return pattern_strings, pf_one_df, times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'UrbanAir'\n",
    "database_path = r\"../data/UTDATABASE/utd_20221222_0226/database.csv\"\n",
    "database_df = pd.read_csv(database_path)\n",
    "database_df.fillna(0, inplace=True)\n",
    "# database_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full data run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_default_support = 500\n",
    "max_default_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = f'../output_temp/{dataset_name}/FULLDATA/ufp_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "mine_patterns(database_df, output_folder_path, min_default_support, max_default_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With only traffic or sensor data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_columns = ['Altitude', 'Temperature', 'Humidity', 'Rainfall', 'WindGust', 'WindSpeed', 'WindCos', 'WindSin', 'UV']\n",
    "traffic_columns = ['Person', 'Car', 'Bus', 'Motorbike', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_columns(database_df, columns):\n",
    "    column_names = []\n",
    "    for c in database_df.columns:\n",
    "        for l in columns:\n",
    "            if l in c:\n",
    "                column_names.append(c)\n",
    "                break\n",
    "    #print(column_names)\n",
    "    new_database_df = database_df.drop( column_names, axis=1 ).reset_index(drop=True)\n",
    "    #new_database_df.info()\n",
    "    return new_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_traffic_support = 10\n",
    "max_traffic_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_traffic_folder_path = f'../output_temp/{dataset_name}/TRAFFIC/ufp_traffic_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_traffic_folder_path, exist_ok=True)\n",
    "\n",
    "traffic_database_df = delete_columns(database_df, sensor_columns)\n",
    "mine_patterns(traffic_database_df, output_traffic_folder_path,\n",
    "            min_traffic_support, max_traffic_period)\n",
    "# del traffic_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_sensor_folder_path = '../data/UTDatabase/Mining/ufp_sensor_' + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "# os.makedirs(output_sensor_folder_path, exist_ok=True)\n",
    "\n",
    "# sensor_database_df = delete_columns(database_df, traffic_columns)\n",
    "# mine_patterns(sensor_database_df, output_sensor_folder_path, min_support, max_period)\n",
    "# del sensor_database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with specific columns for traffic relation finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to see if for extreme cases such as PM10 lv4, is there a hidden pattern among them?\n",
    "Each columns shall be selected, and the dataframe will be filtered based on that col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_spec_support = 3\n",
    "max_spec_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_database_df = delete_columns(database_df, sensor_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_unhelpful_columns = ['CO', 'NO2', 'SO2']\n",
    "# dropping_unhelpful_columns = []\n",
    "# for col in traffic_database_df.columns:\n",
    "#     for ex in exclude_unhelpful_columns:\n",
    "#         if (ex in col) :\n",
    "#             dropping_unhelpful_columns.append(col)\n",
    "# new_traffic_database_df = traffic_database_df.drop( dropping_unhelpful_columns , axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in new_traffic_database_df.columns:\n",
    "#     if ('HIGH' in col) or ('lv4' in col) or ('lv5' in col) or ('HourTriple' in col): \n",
    "#         sub_df = new_traffic_database_df[ new_traffic_database_df[col] > 0 ].copy(True).reset_index(drop=True)\n",
    "#         output_col_path = os.path.join(output_speccol_path, col)\n",
    "#         mine_patterns(sub_df, output_col_path, min_spec_support, max_spec_period )\n",
    "#         del sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with each one AQI item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration, isolate one AQI item from the rest (delete all other AQI items), to find pattern specifically for that item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_AQI_support = 1\n",
    "max_AQI_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_label_columns = [ col for col in traffic_database_df.columns if 'AQI' in col ]\n",
    "aqi_label_columns\n",
    "\n",
    "aqi_df = traffic_database_df[ aqi_label_columns ].copy(True)\n",
    "\n",
    "custom_aqi_database_df = traffic_database_df.drop( aqi_label_columns, axis=1 )\n",
    "custom_aqi_database_df = custom_aqi_database_df.drop( [ c for c in custom_aqi_database_df.columns if 'WeekDay' in c ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_aqi_path = f\"../output_temp/{dataset_name}/AQI/ufp_aqi_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_aqi_path, exist_ok=True)\n",
    "\n",
    "for aqicol in aqi_label_columns:\n",
    "    print(aqicol)\n",
    "    sub_df = custom_aqi_database_df.copy(True)\n",
    "    sub_df[aqicol] = aqi_df[aqicol].copy(True)\n",
    "    sub_df = sub_df[ sub_df[aqicol] > 0 ].sort_values(by=['Time'], ignore_index=True)               # prob = 0 mean not exist, dont use\n",
    "    if (sub_df.shape[0] == 0): continue\n",
    "    output_col_path = os.path.join(output_aqi_path, aqicol)\n",
    "    mine_patterns(sub_df, output_col_path, min_AQI_support, max_AQI_period )\n",
    "    del sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining with only high traffic signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hightraffic_support = 15\n",
    "max_hightraffic_period = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_traffic_database_df = traffic_database_df\n",
    "# for col in traffic_columns:\n",
    "#     high_traffic_database_df = high_traffic_database_df[ high_traffic_database_df[ col + '_LOW' ] == 0 ]\n",
    "# high_traffic_database_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_hightraffic_path = r\"../output_temp/HIGHTRAFFIC/ufp_hightraffic_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "# os.makedirs(output_hightraffic_path, exist_ok=True)\n",
    "\n",
    "# mine_patterns(high_traffic_database_df, output_hightraffic_path, min_hightraffic_support, max_hightraffic_period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining with each traffic item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process to each AQI item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_eachtraffic_support = 1.5\n",
    "max_eachtraffic_period = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_label_columns = []\n",
    "for tcol in traffic_columns:\n",
    "    for col in traffic_database_df.columns:\n",
    "        if (tcol in col):\n",
    "            traffic_label_columns.append(col)\n",
    "\n",
    "trafficonly_df = traffic_database_df[ traffic_label_columns ].copy(True)\n",
    "traffic_excluded_database_df = traffic_database_df.drop( traffic_label_columns, axis=1 )\n",
    "# traffic_excluded_database_df = traffic_excluded_database_df.drop( [ c for c in traffic_excluded_database_df.columns if 'WeekDay' in c ], axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_excluded_database_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_eachtraffic_path = f\"../output_temp/{dataset_name}/EACHTRAFFIC/ufp_eachtraffic_\" + datetime.datetime.now().strftime(format=\"%Y%m%d_%H%M\")\n",
    "os.makedirs(output_eachtraffic_path, exist_ok=True)\n",
    "\n",
    "for c in traffic_label_columns:\n",
    "    print(c)\n",
    "    sub_df = traffic_excluded_database_df.copy(True)\n",
    "    sub_df[c] = trafficonly_df[c].copy(True)\n",
    "    sub_df = sub_df[ sub_df[c] > 0 ].sort_values(by=['Time'], ignore_index=True)               # prob = 0 mean not exist, dont use\n",
    "    if (sub_df.shape[0] == 0): continue\n",
    "    output_col_path = os.path.join(output_eachtraffic_path, c)\n",
    "    mine_patterns(sub_df, output_col_path, min_eachtraffic_support, max_eachtraffic_period )\n",
    "    del sub_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb3de00e07e345391905e8474aaa8d141fe6b310547503c17a51563c220f85dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
