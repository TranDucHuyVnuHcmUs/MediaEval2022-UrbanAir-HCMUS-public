{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodic frequent-pattern discovery (with time measurement)\n",
    "Using algorithm from this paper: https://www.researchgate.net/publication/356825094_Discovering_Periodic-Frequent_Patterns_in_Uncertain_Temporal_Databases?enrichId=rgreq-1de5ac4c085dd4a641f85dda9c527a37-XXX&enrichSource=Y292ZXJQYWdlOzM1NjgyNTA5NDtBUzoxMTQzMTI4MTA4MzQ2MzY1M0AxNjYyNTk3MzM3NTUz&el=1_x_3&_esc=publicationCoverPdf\n",
    "\n",
    "The algorithm is the same as the original pfpattern.ipynb, but seperated for custom time measurements (and protect ourselves from screwing the original file).\n",
    "\n",
    "This script assumes there're preprocessed datasets such as T40I10D100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib\n",
    "import sortednp\n",
    "import datetime\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import timeit\n",
    "import time\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find frequent items (1-pattern)\n",
    "\n",
    "Both sum of prob and max of time difference (period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_columns = ['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database(database_df,\n",
    "                    min_support, max_period):\n",
    "    expsup_one_df = database_df.sum(axis=0)\n",
    "\n",
    "    period_one_df = expsup_one_df.copy(deep=True)\n",
    "    min_time, max_time = database_df['Time'].min(), database_df['Time'].max()\n",
    "    for col in database_df.columns:\n",
    "        if (col == 'Time'): continue\n",
    "        ser = database_df['Time'][ database_df[col] > 0 ]\n",
    "        #print(ser.min(), ser.diff().max(), ser.max())\n",
    "        period_one_df[col] = max( ser.min() - min_time, ser.diff().max(), max_time - ser.max() )\n",
    "\n",
    "    pf_one_items = [ [col, expsup_one_df[col], period_one_df[col]] \n",
    "                        for col in database_df.columns \n",
    "                        if (col != 'Time') and (expsup_one_df[col] >= min_support) and (max_period >= period_one_df[col])  ]\n",
    "    pf_one_df = pd.DataFrame(pf_one_items, columns = ['Item', 'ExpSup', 'MaxPeriod'])\n",
    "    pf_one_df.sort_values(by=['ExpSup', 'MaxPeriod'], ignore_index=True, ascending=False, inplace=True)\n",
    "    pf_one_df\n",
    "\n",
    "    items = pf_one_df['Item'].unique().tolist()\n",
    "    mining_df = database_df[ ['Time'] + items ]\n",
    "\n",
    "    #print('Items = ', items)\n",
    "    return mining_df, items, pf_one_df, min_time, max_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the code are taken from: https://github.com/Likhitha-palla/UPFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, item, children, parent, probability=0):\n",
    "        self.item = item\n",
    "        self.probability = probability              # expSupCap of the path/pattern\n",
    "        self.children = children\n",
    "        self.parent = parent\n",
    "        #self.times = []\n",
    "        self.times = np.ndarray(shape=(0))\n",
    "\n",
    "    def addChild(self, node):\n",
    "        if (node.item not in self.children):\n",
    "            self.children[node.item] = []\n",
    "        self.children[node.item] = node\n",
    "        node.parent = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    def __init__(self, items):\n",
    "        self.nodecounts = 0\n",
    "        self.root = Node(-1, {}, None)\n",
    "        self.items = items\n",
    "        self.nodelists = {}     # item  : [ nodes of this item ]\n",
    "        for i in items:\n",
    "            self.nodelists[i] = []\n",
    "        # self.nodelists = [ [] for i in range(len(items)) ]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(matrix, time_col, items, algo_mode: int = 1):\n",
    "    tree = Tree(items)    \n",
    "    print(\"Algo mode = \", algo_mode, \" (1 = min, 2 = max)\")\n",
    "    \n",
    "    x_indexes, y_indexes = np.where( matrix > 0 )\n",
    "    x_idx = 0\n",
    "    y_idx = 0\n",
    "    i = 0\n",
    "    node = tree.root\n",
    "    item = items[y_indexes[0]]\n",
    "    expCap = 1\n",
    "        \n",
    "        \n",
    "    while (i < x_indexes.shape[0]):\n",
    "        x_idx = x_indexes[i]        # row\n",
    "        y_idx = y_indexes[i]        # col\n",
    "\n",
    "        item = items[y_idx]\n",
    "        #item = y_idx                # now use the index as the item itself (the list of item names are present in the tree already)\n",
    "        prob = matrix[x_idx][y_idx]\n",
    "        \n",
    "        if (item in node.children):\n",
    "            node = node.children[item]\n",
    "        else:\n",
    "            new_node = Node(item, {}, node)\n",
    "            tree.nodecounts += 1\n",
    "            node.addChild(new_node)\n",
    "            node = new_node\n",
    "            tree.nodelists[item].append(node)\n",
    "        node.probability += prob*expCap                # multiply by maximum prob of previous items in this transaction.\n",
    "        \n",
    "        if (algo_mode == 1):\n",
    "            expCap = min(expCap, prob)\n",
    "        elif (algo_mode == 2):\n",
    "            expCap = max(expCap, prob)                    # changing to max, see if it's better\n",
    "    \n",
    "        i += 1\n",
    "        if (i >= x_indexes.shape[0]) or (x_idx != x_indexes[i]):               # the current row is done, the node is the leaf\n",
    "            node.times = np.append( node.times, time_col[x_idx] )      \n",
    "            node = tree.root            # reset back to root\n",
    "            expCap = 1\n",
    "        #progress_bar.update(1)\n",
    "    \n",
    "    #progress_bar.close()\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove item from tree entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_item(tree: Tree, item):\n",
    "    for node in tree.nodelists[item]:\n",
    "        node.parent.times = np.concatenate([node.parent.times, node.times])\n",
    "        \n",
    "        #sortednp.merge( node.parent.times, node.times ) \n",
    "        #node.parent.times += node.times\n",
    "        node.parent.children[item] = None\n",
    "        del node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UPFP-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of patterns shall look like this: [ [ items ], support, period ]\n",
    "\n",
    "The string produced for report for each pattern: {items} [support, period]\n",
    "\n",
    "For example: \"AQI_O3_MED, Motorbike_MED\" [245, 154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_path(node: Node):\n",
    "    path = []\n",
    "    p = node\n",
    "    while (p.parent.item != -1):\n",
    "        path.append(p.parent.item)\n",
    "        p = p.parent\n",
    "    path.reverse()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_period(times: np.ndarray, min_time: float, max_time: float):\n",
    "    if (times is None) or (len(times) == 0): return 9999999\n",
    "    ts = np.sort(times)\n",
    "    period = max( min(ts) - min_time, max_time - max(ts) )\n",
    "    if (len(ts) > 1):\n",
    "        period = max(period, np.max(np.diff(ts)))\n",
    "    # for i in range(len(ts)-1):\n",
    "    #     difference = ts[i+1] - ts[i]\n",
    "    #     period = max(period, difference)\n",
    "    return period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_items(cond_patterns, cond_ts, cond_sups, min_support, max_period, min_time, max_time):\n",
    "    item_sup_per = { }\n",
    "    for i in range(len(cond_patterns)):\n",
    "        for item in cond_patterns[i]:\n",
    "            if (item not in item_sup_per):  \n",
    "                item_sup_per[item] = [ 0 , np.ndarray(shape=(0)) ]          # support/prob, period\n",
    "            item_sup_per[item][0] += cond_sups[i]\n",
    "            item_sup_per[item][1] = np.concatenate( [ item_sup_per[item][1], cond_ts[i] ] )  # appending lists\n",
    "    for item in item_sup_per:\n",
    "        item_sup_per[item][1] = find_period(item_sup_per[item][1], min_time, max_time)\n",
    "    freq_item_dict = { key: value for key, value in item_sup_per.items() if (value[0] >= min_support) and (value[1] <= max_period) }\n",
    "    freq_item_dict = dict(sorted(freq_item_dict.items(), key=lambda item: (item[1][0], item[1][1])))\n",
    "\n",
    "    return freq_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition_pattern_base(tree: Tree, item, min_support, max_period, min_time, max_time):\n",
    "    cond_patterns = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_times = [ None for i in range(len(tree.nodelists[item])) ]\n",
    "    cond_sups = np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    #[ None for i in range(len(tree.nodelists[item])) ]\n",
    "    \n",
    "    i = 0\n",
    "    for node in tree.nodelists[item]:\n",
    "        cond_patterns[i] = traverse_path(node)\n",
    "        cond_times[i] = node.times \n",
    "        cond_sups[i] = node.probability\n",
    "        i += 1\n",
    "    freq_item_sup_per = find_frequent_items(cond_patterns, cond_times, cond_sups, min_support, max_period, min_time, max_time)\n",
    "    \n",
    "    new_patterns, new_times, new_sups = [], [], np.ndarray(shape=(len(tree.nodelists[item])))\n",
    "    count = 0\n",
    "    for p in cond_patterns:\n",
    "        p1 = [ item for item in p if item in freq_item_sup_per ]\n",
    "        if (len(p1) > 0):\n",
    "            p1 = sorted(p1, key=lambda item: (freq_item_sup_per[item][0], freq_item_sup_per[item][1]), reverse=True )\n",
    "            new_patterns.append(p1)\n",
    "            new_times.append(cond_times[count])\n",
    "            #new_sups.append(cond_sups[count])\n",
    "            new_sups[count] = cond_sups[count]\n",
    "        count += 1\n",
    "    new_sups = new_sups[0 : count]\n",
    "    return freq_item_sup_per, new_patterns, new_times, new_sups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_condition(tree: Tree,transaction,times,sup):\n",
    "    node=tree.root\n",
    "    for item in transaction:\n",
    "        if item not in node.children:\n",
    "            new_node=Node(item,{}, node)\n",
    "            node.addChild(new_node)\n",
    "            # if item not in tree.nodelists:\n",
    "            #     tree.nodelists[ item ] = []\n",
    "            tree.nodelists[item].append(new_node)            \n",
    "        node = node.children[item] \n",
    "        node.probability += sup           \n",
    "    node.times = np.concatenate( [node.times, times] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the list of all pattern satisfying the constrains. \n",
    "def upfp_growth(tree: Tree, prefix, \n",
    "                min_support, max_period, \n",
    "                min_time, max_time):\n",
    "    mined_patterns = []\n",
    "\n",
    "    for i in range(len(tree.items)-1, -1, -1):\n",
    "        item = tree.items[i]\n",
    "        newprefix = prefix + [item]\n",
    "        expsup = 0\n",
    "        for node in tree.nodelists[item]:\n",
    "            expsup += node.probability\n",
    "        if (expsup >= min_support):\n",
    "            freq_item_sup_per, cond_patterns, cond_times, cond_sups = make_condition_pattern_base(tree, item, \n",
    "                                                                                min_support, max_period, \n",
    "                                                                                min_time, max_time)\n",
    "            cond_tree = Tree(list(freq_item_sup_per.keys()))\n",
    "            for p in range(len(cond_patterns)):\n",
    "                add_transaction_condition(cond_tree, cond_patterns[p], cond_times[p], cond_sups[p])\n",
    "            if (len(cond_patterns) > 0):\n",
    "                mined_patterns += upfp_growth(cond_tree, newprefix, min_support, max_period, min_time, max_time ) \n",
    "            else:\n",
    "                # if no more items to search: stop and return.\n",
    "                mined_patterns.append(np.array(newprefix, dtype=np.int16))\n",
    "        remove_item(tree, item)\n",
    "    return mined_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalculate actual Expected support for one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_sort(array, count_func):\n",
    "    max_count = 0\n",
    "    for i in range(len(array)):\n",
    "        max_count = max(max_count, count_func(array[i]))\n",
    "\n",
    "    # the usable count is [0, max_count], inclusive, hence (max_count + 1) different counts\n",
    "    # we need h[max_count+1] be the end index of sorted array, for iterating the last count's indexes\n",
    "    count_list_heads = [ 0 for i in range(max_count + 2)]          \n",
    "    for i in range(len(array)):\n",
    "        count_list_heads[ count_func(array[i]) ] += 1\n",
    "    \n",
    "    # cumulative sum\n",
    "    for i in range(max_count + 1):\n",
    "        count_list_heads[i + 1] += count_list_heads[i]\n",
    "    \n",
    "    sorted_array = [ None for i in range(len(array)) ]\n",
    "    for i in range(len(array)):\n",
    "        count = count_func(array[i])\n",
    "        count_list_heads[ count ] -= 1\n",
    "        sorted_array[ count_list_heads[ count ] ] = array[i] \n",
    "\n",
    "    return sorted_array, count_list_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def pattern_priority(ptn):\n",
    "    return np.min(ptn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_expSup(pattern_matrix: np.ndarray):\n",
    "    probabilities = np.zeros(pattern_matrix.shape[0])\n",
    "    for i in range(pattern_matrix.shape[0]):\n",
    "        probabilities[i] = np.prod(pattern_matrix[i, :])\n",
    "    # probabilities = np.prod(pattern_matrix, axis=1, keepdim=False)\n",
    "    return probabilities, np.sum(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def find_final_period(times: np.ndarray, \n",
    "                    probabilities: np.ndarray, \n",
    "                    min_time: float, max_time: float):\n",
    "    period = 0\n",
    "    times = times.copy()\n",
    "    times = times[ probabilities > 0 ]\n",
    "    # times = times.to('cuda:0')\n",
    "    if (times is None) or (times.shape[0] == 0): \n",
    "        period = 999999\n",
    "    else:\n",
    "        period = max( np.min(times) - min_time, max_time - np.max(times) )\n",
    "    if (len(times) > 1):\n",
    "        period = max( period, np.max(np.diff(times)) )\n",
    "    return period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_patterns(mined_patterns, mining_df, min_time, max_time, min_support, max_period):\n",
    "    \n",
    "    sorted_patterns, length_heads = counting_sort(mined_patterns, lambda ptn: len(ptn))\n",
    "    for length in range(1, len(length_heads) - 2):\n",
    "        sorted_patterns[ length_heads[length] : length_heads[length+1] ] = sorted(\n",
    "                                    sorted_patterns[length_heads[length] : length_heads[length+1]],\n",
    "                                    key = lambda p: pattern_priority(p) )\n",
    "\n",
    "    filtered_patterns = []\n",
    "    print('Potential max pattern length = ', len(length_heads) - 2)\n",
    "    \n",
    "    tsr = mining_df.drop(columns=['Time'], axis=1).to_numpy()\n",
    "    time_tsr = mining_df['Time'].to_numpy()\n",
    "    #tsr = torch.FloatTensor( mining_df.drop(columns=['Time'], axis=1).to_numpy() ).detach().to(device)      # for faster calculation for big dataset\n",
    "    #time_tsr = torch.LongTensor( mining_df['Time'].to_numpy()).detach().to(device)\n",
    "    \n",
    "\n",
    "    # binary search\n",
    "    min_length = 1\n",
    "    max_length = len(length_heads) - 2\n",
    "    length = 0\n",
    "    start_pattern_idx = -1\n",
    "    max_pattern_start_idx = -1\n",
    "\n",
    "\n",
    "    # quick search to reduce the space needed to scan.\n",
    "    length = 1\n",
    "    while (length <= max_length):\n",
    "        pattern_idx = length_heads[length]\n",
    "        start_pattern_idx = -1\n",
    "        while (pattern_idx < length_heads[length + 1]):\n",
    "            ptn = sorted_patterns[pattern_idx]  \n",
    "            #sub_tsr = torch.index_select(input=tsr, dim=1, index=torch.LongTensor(ptn).to(device))\n",
    "            sub_tsr = tsr[ :, ptn ]\n",
    "            probabilities, expSup = find_expSup(sub_tsr)\n",
    "            period = find_final_period(time_tsr, probabilities,\n",
    "                                min_time, max_time)\n",
    "            if (expSup >= min_support) and (period <= max_period):       \n",
    "                start_pattern_idx = pattern_idx\n",
    "                break\n",
    "            #torch.cuda.empty_cache() \n",
    "            if (length_heads[length + 1] - length_heads[length] == 1):\n",
    "                break\n",
    "            pattern_idx += int((np.log2(length_heads[length + 1] - length_heads[length])))\n",
    "            \n",
    "        if (start_pattern_idx > -1):\n",
    "            print(\"Found the pattern with length = \", length)\n",
    "            max_pattern_length = length\n",
    "            max_pattern_start_idx = length_heads[length]\n",
    "            min_length = length + 1\n",
    "        length *= 2\n",
    "\n",
    "\n",
    "    # this search find the most possible length, but you must pass it another time to have all patterns with such length.\n",
    "    while (min_length <= max_length):\n",
    "        start_pattern_idx = -1\n",
    "        length = (min_length + max_length) // 2\n",
    "        print(\"Checking potential patterns with length = \", length)\n",
    "\n",
    "        for pattern_idx in tqdm(range( length_heads[length], length_heads[ length + 1 ]), desc=\"pattern\" ):\n",
    "            ptn = sorted_patterns[pattern_idx]\n",
    "            sub_tsr = tsr[ :, ptn ]\n",
    "            probabilities, expSup = find_expSup(sub_tsr)\n",
    "            period = find_final_period(time_tsr, probabilities,\n",
    "                                min_time, max_time)\n",
    "\n",
    "            if (expSup >= min_support) and (period <= max_period):       \n",
    "                start_pattern_idx = pattern_idx\n",
    "                break\n",
    "\n",
    "        if (start_pattern_idx > -1):\n",
    "            print(\"Found the pattern with length = \", length)\n",
    "            max_pattern_length = length\n",
    "            max_pattern_start_idx = start_pattern_idx\n",
    "            min_length = length + 1\n",
    "        else:\n",
    "            max_length = length - 1\n",
    "        \n",
    "\n",
    "    print(\"Now calculating all patterns with max length...\")\n",
    "    for pattern_idx in tqdm(range( max_pattern_start_idx, length_heads[ max_pattern_length + 1 ]), desc=\"pattern\" ):\n",
    "        ptn = sorted_patterns[pattern_idx]\n",
    "        sub_tsr = tsr[ :, ptn ]\n",
    "        probabilities, expSup = find_expSup(sub_tsr)\n",
    "        period = find_final_period(time_tsr, probabilities,\n",
    "                            min_time, max_time)\n",
    "        if (expSup >= min_support) and (period <= max_period):          \n",
    "            filtered_patterns.append( [ ptn, expSup, period ] ) \n",
    "        \n",
    "\n",
    "    filtered_patterns.sort( key = lambda p: (len(p[0]), -p[2], p[1]), reverse=True )\n",
    "    return filtered_patterns, max_pattern_length, len(length_heads) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_strings_from_patterns(patterns, item_list):\n",
    "    pattern_strings = []\n",
    "    for ptn in patterns:\n",
    "        item_str = \"\"\n",
    "        for i in range(len(ptn[0]) - 1):\n",
    "            item_str += item_list[ptn[0][i]] + ', '\n",
    "        item_str += item_list[ptn[0][ len(ptn[0]) - 1 ]]\n",
    "        item_str += ':[' + str(ptn[1]) + ', ' + str(ptn[2]) + ']\\n' \n",
    "        pattern_strings.append( item_str )\n",
    "    return pattern_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_patterns(mined_patterns, mining_df, item_list, min_time, max_time, min_support, max_period):\n",
    "\n",
    "    final_patterns, max_pattern_length, potential_max_length = filter_patterns(mined_patterns, mining_df, \n",
    "                                min_time, max_time, min_support, max_period)\n",
    "    #print('Max pattern length: ', max_pattern_length)\n",
    "    pattern_strings = make_strings_from_patterns(final_patterns, item_list)\n",
    "    if (len(pattern_strings) > 0): print('Most periodic pattern = ', pattern_strings[0])\n",
    "    return pattern_strings, final_patterns, max_pattern_length, potential_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(output_folder_path: str, \n",
    "                pattern_strings, pf_one_df, database_path, min_support, max_period):\n",
    "    os.makedirs(output_folder_path, exist_ok=True)\n",
    "    pf_one_df.to_csv(os.path.join(output_folder_path,'fp_pattern_one.csv'))\n",
    "    with open(os.path.join(output_folder_path, 'patterns.txt'), 'w') as f:\n",
    "        f.writelines(pattern_strings)\n",
    "        f.close()\n",
    "    with open(os.path.join(output_folder_path, 'setting.json'), 'w') as f:\n",
    "        f.write( json.dumps({ 'data_path': database_path,  'min_support': min_support, 'max_period': max_period }, indent=4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual run with time measurement\n",
    "\n",
    "Since this algorithm is deterministic (no random operation), running them many times should yield the same result. Hence, we only care about exporting one output only, but run many times to have an average runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to measure time and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable_columns = [ 'Mode', 'MinSup', 'MaxPer', \n",
    "                'PFItemCount', \n",
    "                'prepare_database', 'make_transaction', 'build_tree', 'upfp_growth', 'produce_pattern', \n",
    "                'max_pattern_length', 'potential_max_length', 'potential_pattern_count',\n",
    "                'size_tree' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_patterns(database_df, output_folder_path, \n",
    "                min_support, max_period, algo_mode: int = 1, export: bool = True):\n",
    "    \n",
    "    times = [ 0 for i in range(5) ]\n",
    "    \n",
    "    times[0] = timeit.default_timer()\n",
    "    mining_df, item_list, pf_one_df, min_time, max_time = prepare_database(database_df, min_support, max_period)\n",
    "    times[0] = timeit.default_timer() - times[0]\n",
    "    \n",
    "    if (len(item_list) == 0): \n",
    "        return [], pf_one_df, None\n",
    "        \n",
    "    times[2] = timeit.default_timer()\n",
    "    #tree = build_tree(transactions, item_list, algo_mode)\n",
    "    time_col = mining_df['Time'].copy().to_numpy()         # to ensure no editing shall affect the main data\n",
    "    npmatrix = mining_df.drop(columns=['Time'], axis=1).to_numpy()\n",
    "    tree = build_tree(npmatrix, time_col, list(range(len(item_list))), algo_mode)\n",
    "    times[2] = timeit.default_timer() - times[2]\n",
    "    size_tree = tree.nodecounts\n",
    "    print(\"Tree node count = \", size_tree)\n",
    "\n",
    "    times[3] = timeit.default_timer()\n",
    "    mined_patterns = upfp_growth(tree, [], min_support, max_period, min_time, max_time )\n",
    "    times[3] = timeit.default_timer() - times[3]\n",
    "    potential_pattern_counts = len(mined_patterns)\n",
    "    print(\"Algo time = \", times[3])\n",
    "\n",
    "    max_pattern_length, potential_max_length = -1, -1\n",
    "    times[4] = timeit.default_timer()\n",
    "    if (export):\n",
    "        pattern_strings, _, max_pattern_length, potential_max_length = produce_patterns(mined_patterns, mining_df, item_list,\n",
    "                                                            min_time, max_time, min_support, max_period)\n",
    "    else: pattern_strings = \"\"\n",
    "    times[4] = timeit.default_timer() - times[4]\n",
    "\n",
    "    times = [algo_mode, min_support, max_period, len(item_list) ] + times + [max_pattern_length, potential_max_length, potential_pattern_counts, size_tree]\n",
    "    \n",
    "    return pattern_strings, pf_one_df, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(database_df:pd.DataFrame, database_path:str,\n",
    "                output_path: str, fulltimetable_path:str,\n",
    "                min_supports, max_periods,\n",
    "                iteration_count: int = 5,\n",
    "                algo_mode: int = 1,\n",
    "                export: bool = True):\n",
    "    time_folder_path = os.path.join(output_path, 'times/')\n",
    "    os.makedirs(time_folder_path, exist_ok=True)\n",
    "    timetable = pd.DataFrame( columns=timetable_columns )\n",
    "    fulltimetable = pd.read_csv(fulltimetable_path)\n",
    "    \n",
    "    pattern_strings = []\n",
    "    for min_sp in min_supports:\n",
    "        for max_per in max_periods:\n",
    "            print('\\n=====\\nMin support, max period = ', (min_sp, max_per))       \n",
    "            outpath = os.path.join(output_path, 's' + str(min_sp) + '_p' + str(max_per))\n",
    "            for i in range(iteration_count):\n",
    "                pattern_strings, pf_one_df, times = mine_patterns(database_df, outpath, \n",
    "                                        min_sp, max_per, algo_mode, export=export)\n",
    "                if (times is None): continue\n",
    "                timetable.loc[ timetable.shape[0] ] = times\n",
    "                timetable.to_csv(time_folder_path + 'times.csv', index=False)\n",
    "                fulltimetable.loc[ fulltimetable.shape[0] ] = times\n",
    "                fulltimetable.to_csv(fulltimetable_path, index=False)\n",
    "                print(\"...Algo time of iteration \" + str(i) + \" = \" + str(times[7]))\n",
    "            \n",
    "            export_results(outpath, pattern_strings, pf_one_df, database_path, min_sp, max_per)\n",
    "\n",
    "    with open(os.path.join(time_folder_path, \"setting.json\"), 'w') as f:\n",
    "        f.write( json.dumps( { 'input_path': database_path, \n",
    "                                'min_supports': min_supports, \n",
    "                                'max_periods': max_periods,\n",
    "                                'algo_mode': algo_mode \n",
    "                            }, indent=4 ) )\n",
    "    return timetable, fulltimetable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"UrbanAir\": r\"../data/UTDATABASE/utd_20221222_0226/label_timeconverted.csv\",\n",
    "    \"T10I4D200K\": r\"../data/others/T10I4D200K/T10I4D200K.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"UrbanAir\"\n",
    "database_path = datasets[database_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = pd.read_csv(database_path)\n",
    "database_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test with different support and period constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_default_support = 500\n",
    "# max_default_period = 1000\n",
    "iteration_count = 1\n",
    "min_supports = []\n",
    "max_periods = []\n",
    "algo_mode = 2       # 1 = min, 2 = max\n",
    "\n",
    "base = 2\n",
    "for i in range(10, 0, -1):    \n",
    "    min_supports.append(pow(base, i))\n",
    "for i in range(10, 12):\n",
    "    max_periods.append(pow(base, i))\n",
    "\n",
    "print(min_supports)\n",
    "print(max_periods)\n",
    "print(iteration_count)\n",
    "print(algo_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_path = f'../output_temp/{database_name}/TIME_V2/ufp_' + datetime.datetime.now().strftime(format=\"%m%d_%H%M\")\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "fulltimetable_path = f\"../output_temp/{database_name}/TIME_V2/times.csv\"\n",
    "if not (os.path.exists(fulltimetable_path)):\n",
    "    timetable = pd.DataFrame( columns= timetable_columns )\n",
    "    timetable.to_csv(fulltimetable_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for initializating numba\n",
    "mine_patterns(database_df, \"\", 4096, 128, algo_mode, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetable, fulltimetable = measure_time(database_df, database_path,\n",
    "                            output_folder_path, fulltimetable_path,\n",
    "                            min_supports, max_periods, iteration_count, algo_mode, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timetable, fulltimetable = measure_time(database_df, database_path,\n",
    "#                             output_folder_path, fulltimetable_path,\n",
    "#                             min_supports, max_periods, 2, algo_mode, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timetable, fulltimetable = measure_time(database_df, database_path,\n",
    "#                             output_folder_path, fulltimetable_path,\n",
    "#                             min_supports, max_periods, 1, 3 - algo_mode, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fulltimefolder_path = f\"../output_temp/{database_name}/TIME_V2/\"\n",
    "fulltimetable = pd.read_csv(fulltimefolder_path + 'times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = { \n",
    "            'min_supports': fulltimetable['MinSup'].unique().tolist(),\n",
    "            'max_periods': fulltimetable['MaxPer'].unique().tolist() \n",
    "            }\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_time_groupby = fulltimetable.groupby(by=['Mode', 'MinSup', 'MaxPer'])\n",
    "grouped_time_df = grouped_time_groupby.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_runtime_df = grouped_time_df[ ['PFItemCount', 'upfp_growth'] ]\n",
    "algo_runtime_df.reset_index(inplace=True)\n",
    "algo_runtime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_by_pfitemcount_timedf = timetable.sort_values(by=['PFItemCount'],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_colors = [' ', 'bo-', 'ro-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minsup_output_path = os.path.join(fulltimefolder_path, 'minsups/')\n",
    "os.makedirs(minsup_output_path, exist_ok=True)\n",
    "\n",
    "for min_sup in settings['min_supports']:\n",
    "    for algo_mode in range(1,3):\n",
    "        sub_df = algo_runtime_df[ (algo_runtime_df['Mode'] == algo_mode) & (algo_runtime_df['MinSup'] == min_sup) ]\n",
    "        max_pers = sub_df['MaxPer'].to_numpy()\n",
    "        times = sub_df['upfp_growth'].to_numpy()\n",
    "        plt.plot(max_pers, times, mode_colors[algo_mode], label='Mode'+str(algo_mode))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Max periods')\n",
    "    plt.ylabel('Time (sec)')\n",
    "    plt.title('Time of algo by min support = ' + str(min_sup))\n",
    "    \n",
    "    plt.xscale('log', base=2)\n",
    "    plt.yscale('log', base=2)\n",
    "    min_log_times, max_log_times = int(np.floor(np.min(np.log2(times)))), int(np.ceil(np.max(np.log2(times))))\n",
    "    min_log_per, max_log_per = int(np.floor(np.min(np.log2(max_pers)))), int(np.ceil(np.max(np.log2(max_pers))))\n",
    "    plt.xticks([ pow(2, p) for p in range(min_log_per, max_log_per + 1) ])\n",
    "    plt.yticks([ pow(2, p) for p in range(min_log_times, max_log_times + 1) ])\n",
    "    #plt.show()\n",
    "\n",
    "    plt.savefig(os.path.join(minsup_output_path, 'minSup' + str(min_sup) + '.png'))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxper_output_path = os.path.join(fulltimefolder_path, 'maxpers/')\n",
    "os.makedirs(maxper_output_path, exist_ok=True)\n",
    "\n",
    "for max_per in settings['max_periods']:\n",
    "    print(max_per)\n",
    "    for algo_mode in range(1,3):\n",
    "        sub_df = algo_runtime_df[ (algo_runtime_df['Mode'] == algo_mode) & (algo_runtime_df['MaxPer'] == max_per) ]\n",
    "        min_sups = sub_df['MinSup'].to_numpy()\n",
    "        times = sub_df['upfp_growth'].to_numpy()\n",
    "        plt.plot(min_sups, times, mode_colors[algo_mode], label='Mode'+str(algo_mode))\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.xlabel('Min support')\n",
    "    plt.ylabel('Time (sec)')\n",
    "\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.yscale('log', base=2)\n",
    "\n",
    "    min_log_times, max_log_times = int(np.floor(np.min(np.log2(times)))), int(np.ceil(np.max(np.log2(times))))\n",
    "    min_log_x, max_log_x = int(np.floor(np.min(np.log2(min_sups)))), int(np.ceil(np.max(np.log2(min_sups))))\n",
    "    plt.xticks([ pow(2, p) for p in range(min_log_x, max_log_x + 1) ])\n",
    "    plt.yticks([ pow(2, p) for p in range(min_log_times, max_log_times + 1) ])\n",
    "\n",
    "    plt.title('Time of algo by max period = ' + str(max_per))\n",
    "    #plt.show()\n",
    "    plt.savefig(os.path.join(maxper_output_path, 'maxPer' + str(max_per) + '.png'))\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3d plot\n",
    "# x_values = algo_runtime_df['MinSup'].to_numpy()\n",
    "# y_values = algo_runtime_df['MaxPer'].to_numpy()\n",
    "# z_values = algo_runtime_df['upfp_growth'].to_numpy()  \n",
    "# fig = plt.figure(10) \n",
    "# ax = plt.axes(projection='3d')\n",
    "# ax.plot_surface(x_values, y_values, z_values)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('study')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb3de00e07e345391905e8474aaa8d141fe6b310547503c17a51563c220f85dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
